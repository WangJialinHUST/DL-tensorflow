{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks: Language Modeling\n",
    "\n",
    "Given words $x_{1},x_{2},...,x_{t}$,a language model will predict the following word $x_{t+1}$ by modeling:\n",
    "\n",
    "$P(x_{t+1}=v_{j}|x_{t},...,x_{1})$\n",
    "\n",
    "which $v_{j}$ is a word in vocabulary.\n",
    "\n",
    "$e^{(t)}=x^{(t)}L$\n",
    "$h^{(t)}=sigmoid(h^{(t-1)}H + e^{(t)}I + b_{1})$\n",
    "$y^{(t)}=softmax(h^{(t)}U + b2)$\n",
    "\n",
    "$P(x_{t+1}=v_{j}|x_{t},...,x_{1})=\\hat{y}_{j}^{(t)}$\n",
    "\n",
    "$x^{(t)}\\in R^{|V|}$ : one-hot row vector representing the index of the current word. \n",
    "\n",
    "$L\\in R^{|V|*d}$ : word embeddings, \n",
    "\n",
    "$H\\in R^{D_{h}*D_{h}}$ : the hidden transformation matrix\n",
    "\n",
    "$I\\in R^{d*D_{h}}$ : the input word to hidden transformation matrix\n",
    "\n",
    "$b_{1} \\in R^{D_{h}}$ : bias\n",
    "\n",
    "$b_{2} \\in R^{|V|}$ : bias\n",
    "\n",
    "$|V|$ : the vocabulary size; $d$ : the word embedding size; $D_{h}$ : the hidden layer dimension\n",
    "\n",
    "### Loss Function:\n",
    "cross entropy :\n",
    "$J^{(t)}(\\theta)=CE(y^{(t)},\\hat{y}^{(t)})=-\\sum_{i=1}^{|V|}y_{i}^{(t)}log\\hat{y}^{(t)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "embed_size = 50\n",
    "hidden_size = 100\n",
    "num_steps = 10\n",
    "max_epochs = 16\n",
    "early_stopping = 2\n",
    "dropout = 0.9\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import calculate_perplexity, get_ptb_dataset, Vocab\n",
    "from utils import ptb_iterator, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Loads starter word-vectors and train/dev/test data.\"\"\"\n",
    "    vocab = Vocab()\n",
    "    vocab.construct(get_ptb_dataset('train'))\n",
    "    encoded_train = np.array(\n",
    "        [vocab.encode(word) for word in get_ptb_dataset('train')],\n",
    "        dtype=np.int32)\n",
    "    encoded_valid = np.array(\n",
    "        [vocab.encode(word) for word in get_ptb_dataset('valid')],\n",
    "        dtype=np.int32)\n",
    "    encoded_test = np.array(\n",
    "        [vocab.encode(word) for word in get_ptb_dataset('test')],\n",
    "        dtype=np.int32)\n",
    "    return encoded_train, encoded_valid, encoded_test, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929589.0 total words with 10000 uniques\n"
     ]
    }
   ],
   "source": [
    "train_set, valid_set, test_set, vocab = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n",
      "  0 27 28 29 30 31 32 33 34 35 36 37 38 27 25 39  0 40 41 42  0 43 32 44]\n"
     ]
    }
   ],
   "source": [
    "print(train_set[1:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add the placeholders\n",
    "def add_placeholders():\n",
    "    input_placeholder = tf.placeholder(tf.int32, shape=[None, num_steps])\n",
    "    label_placeholder = tf.placeholder(tf.int32, shape=[None, num_steps])\n",
    "    initial_state_placeholder = tf.placeholder(tf.float32, shape=[batch_size, hidden_size])\n",
    "    return input_placeholder, label_placeholder, initial_state_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the feed_dict\n",
    "def create_feed_dict(input_placeholder, input_batch, label_placeholder, label_batch, \n",
    "                     initial_state_placeholder, initial_state):\n",
    "    feed_dict = {input_placeholder: input_batch,\n",
    "                label_placeholder:label_batch,\n",
    "                initial_state_placeholder:initial_state}\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### HINTS\n",
    "### You should take care of how to construct the model inputs\n",
    "tf.squeeze(input, squeeze_dims=None, name=None):Removes dimensions of size 1 from the shape of a tensor.\n",
    "\n",
    "tf.split(split_dim, num_split, value, name='split'):Splits a tensor into `num_split` tensors along one dimension.\n",
    "    \n",
    "num_steps: like the $t$ in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_embed_layer(vocab_size, input_placeholder):\n",
    "    with tf.device('/cpu:0'):\n",
    "        embed = tf.get_variable(name=\"Embedding\", shape=[vocab_size, embed_size])\n",
    "        inputs = tf.nn.embedding_lookup(embed, input_placeholder)\n",
    "        inputs = [tf.squeeze(input, squeeze_dims=[1]) for input in tf.split(1, num_steps, inputs)] \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## according the model at the begining of this file\n",
    "def add_model(inputs, vocab_size, initial_state_placeholder):\n",
    "    ###initial state\n",
    "    with tf.variable_scope('RNN') as scope:\n",
    "        #state = tf.zeros([batch_size, hidden_size])\n",
    "        state = initial_state_placeholder\n",
    "        rnn_outputs = []\n",
    "        for tstep, current_input in enumerate(inputs):\n",
    "            if tstep > 0:\n",
    "                scope.reuse_variables()\n",
    "            RNN_H = tf.get_variable('HMatrix', shape=[hidden_size, hidden_size])\n",
    "            RNN_I = tf.get_variable('IMatrix', shape=[embed_size, hidden_size])\n",
    "            RNN_b1 = tf.get_variable('b1', shape=[hidden_size])\n",
    "            state = tf.sigmoid(tf.matmul(state, RNN_H) + tf.matmul(current_input, RNN_I) + RNN_b1)\n",
    "            rnn_outputs.append(state)\n",
    "        final_state = rnn_outputs[-1]\n",
    "    return rnn_outputs, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## add the Projection layer\n",
    "def add_projection_layer(rnn_outputs, vocab_size):\n",
    "    RNN_U = tf.get_variable('UMatrix', shape=[hidden_size, vocab_size])\n",
    "    RNN_b2 = tf.get_variable('b2', shape=[vocab_size])\n",
    "    outputs = [tf.matmul(state, RNN_U) + RNN_b2 for state in rnn_outputs]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.seq2seq import sequence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sequence_loss in module tensorflow.python.ops.seq2seq:\n",
      "\n",
      "sequence_loss(logits, targets, weights, average_across_timesteps=True, average_across_batch=True, softmax_loss_function=None, name=None)\n",
      "    Weighted cross-entropy loss for a sequence of logits, batch-collapsed.\n",
      "    \n",
      "    Args:\n",
      "      logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n",
      "      targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n",
      "      weights: List of 1D batch-sized float-Tensors of the same length as logits.\n",
      "      average_across_timesteps: If set, divide the returned cost by the total\n",
      "        label weight.\n",
      "      average_across_batch: If set, divide the returned cost by the batch size.\n",
      "      softmax_loss_function: Function (inputs-batch, labels-batch) -> loss-batch\n",
      "        to be used instead of the standard softmax (the default if this is None).\n",
      "      name: Optional name for this operation, defaults to \"sequence_loss\".\n",
      "    \n",
      "    Returns:\n",
      "      A scalar float Tensor: The average log-perplexity per symbol (weighted).\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If len(logits) is different from len(targets) or len(weights).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sequence_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## add loss op\n",
    "def add_loss_op(outputs, label_placeholder, vocab_size):\n",
    "    \n",
    "    all_one_weights = num_steps*[tf.ones([batch_size])]\n",
    "    ### TODO len(targets) = len(output) = num_steps!!\n",
    "    ##construct the targets\n",
    "    targets = [tf.squeeze(label_placeholder[:, i]) for i in range(num_steps)]\n",
    "    cross_entropy = tf.nn.seq2seq.sequence_loss(outputs,                                           targets=targets,                                        weights=all_one_weights)\n",
    "    loss = cross_entropy\n",
    "    ''' \n",
    "    \n",
    "    all_ones = [tf.ones([batch_size * num_steps])]\n",
    "    cross_entropy = tf.nn.seq2seq.sequence_loss(\n",
    "        [outputs], [tf.reshape(label_placeholder, [-1])], all_ones, vocab_size)\n",
    "    loss = cross_entropy\n",
    "    '''\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## add training op\n",
    "def add_train_op(loss):\n",
    "    train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evalate the prediction \n",
    "def evaluation(y_pred, label_placeholder):\n",
    "    label_pred = [tf.cast(tf.argmax(value, dimension=1), tf.int32) for key, value  in enumerate(y_pred)]\n",
    "    label_right = [tf.squeeze(label_placeholder[:, i]) for i in range(num_steps)]\n",
    "    correct_pred_num = []\n",
    "    for i in range(num_steps):\n",
    "        correct_pred_num.append(tf.reduce_sum(tf.cast(tf.equal(label_right[i], label_pred[i]), tf.int32)))\n",
    "    correct_pred_num = np.sum(correct_pred_num)\n",
    "    return correct_pred_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate on the validation set\n",
    "def do_evaluation(data_set, sess):\n",
    "    ##TODO\n",
    "    #total_correct_num = []\n",
    "    loss_mean = []\n",
    "    for step, (x, y) in enumerate(ptb_iterator(data_set, batch_size, num_steps)):\n",
    "        if step == 0 :\n",
    "                initial_state = np.zeros([batch_size, hidden_size])\n",
    "        else:\n",
    "            initial_state = final_state_step\n",
    "        #print(step)\n",
    "        input_batch = x\n",
    "        label_batch = y\n",
    "        feed = create_feed_dict(input_placeholder, input_batch, label_placeholder, label_batch,\n",
    "                               initial_state_placeholder, initial_state)\n",
    "        loss_step, final_state_step = sess.run([ loss, final_state], feed)\n",
    "        #total_correct_num.append(eval_correct_step)\n",
    "        loss_mean.append(loss_step)\n",
    "    \n",
    "    #print('Validation Accuracy: %f , Validation Perplexity : %f' %\n",
    "          #(np.sum(total_correct_num)/(batch_size*(step+1)*num_steps), np.exp(np.mean(loss_mean))))\n",
    "    print('Validation Perplexity : %f' %\n",
    "          (np.exp(np.mean(loss_mean))))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Start Training\n",
      "Step 1451: Training Perplexity: 464.571136\n",
      "Start Vailidation\n",
      "Validation Perplexity : 312.959930\n",
      "Epoch 1\n",
      "Start Training\n",
      "Step 1451: Training Perplexity: 258.374359\n",
      "Start Vailidation\n",
      "Validation Perplexity : 245.777573\n",
      "Epoch 2\n",
      "Start Training\n",
      "Step 1451: Training Perplexity: 206.102859\n",
      "Start Vailidation\n",
      "Validation Perplexity : 215.882324\n",
      "Epoch 3\n",
      "Start Training\n",
      "Step 1451: Training Perplexity: 176.621384\n",
      "Start Vailidation\n",
      "Validation Perplexity : 198.346634\n",
      "Epoch 4\n",
      "Start Training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-bab4cf60b612>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m             feed = create_feed_dict(input_placeholder, input_batch, label_placeholder, label_batch,\n\u001b[1;32m     39\u001b[0m                                    initial_state_placeholder, initial_state)\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mloss_mean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m#total_correct.append(eval_correct_step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wang/tensorflow_dl/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 698\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    699\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wang/tensorflow_dl/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    890\u001b[0m     results = self._do_run(handle, fetch_handler.targets(),\n\u001b[1;32m    891\u001b[0m                            \u001b[0mfetch_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m    893\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetch_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wang/tensorflow_dl/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 940\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    941\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/wang/tensorflow_dl/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    945\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wang/tensorflow_dl/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    928\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
    "    \n",
    "    input_placeholder, label_placeholder, initial_state_placeholder = add_placeholders()\n",
    "    \n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    inputs = add_embed_layer(vocab_size, input_placeholder)\n",
    "    \n",
    "    rnn_outputs, final_state = add_model(inputs, vocab_size, initial_state_placeholder)\n",
    "\n",
    "    ## projecttion, output is a List, num_steps of 2-D tensors[batch_size, vocab_size]\n",
    "    outputs = add_projection_layer(rnn_outputs, vocab_size)\n",
    "    ##\n",
    "    #pred = [tf.nn.softmax(value) for key, value in enumerate(outputs)]\n",
    "    #output = tf.reshape(tf.concat(1, outputs), [-1, vocab_size])\n",
    "    #\n",
    "    #eval_correct = evaluation(pred, label_placeholder)\n",
    "    ## add loss op\n",
    "    loss = add_loss_op(outputs, label_placeholder, vocab_size)\n",
    "    ## add training op\n",
    "    train_op = add_train_op(loss)\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    for epoch in range(max_epochs):\n",
    "        print('Epoch %d' %(epoch))\n",
    "        print('Start Training')\n",
    "        \n",
    "        loss_mean = []\n",
    "        total_correct = []\n",
    "        for step, (x, y) in enumerate(ptb_iterator(train_set, batch_size, num_steps)):\n",
    "            if step == 0 :\n",
    "                initial_state = np.zeros([batch_size, hidden_size])\n",
    "            else:\n",
    "                initial_state = final_state_step\n",
    "            #print(step)\n",
    "            input_batch = x\n",
    "            label_batch = y\n",
    "            feed = create_feed_dict(input_placeholder, input_batch, label_placeholder, label_batch,\n",
    "                                   initial_state_placeholder, initial_state)\n",
    "            _, loss_step, final_state_step = sess.run([train_op, loss, final_state], feed)\n",
    "            loss_mean.append(loss_step)\n",
    "            #total_correct.append(eval_correct_step)\n",
    "            #do_evaluation(valid_set, sess)\n",
    "        #print('Step %d: Training Perplexity: %f, Trainings Accuracy : %f ' \n",
    "              #%(step, np.exp(np.mean(loss_mean)), np.sum(total_correct)/(batch_size*num_steps*(step+1))))\n",
    "        print('Step %d: Training Perplexity: %f' %(step, np.exp(np.mean(loss_mean))))\n",
    "        \n",
    "            #do_evaluation(valid_set, sess)\n",
    "                \n",
    "        print('Start Vailidation')\n",
    "        do_evaluation(valid_set, sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_stop = time.time()\n",
    "print('running time:  %f seconds' % (time_stop-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for step, (x, y) in enumerate(ptb_iterator(train_set, batch_size, num_steps)):\n",
    "    input_batch = x\n",
    "    label_batch = y\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(tf.nn.embedding_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(tf.nn.seq2seq.sequence_loss_by_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(tf.variable_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
