{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataReader(object):\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        #self.read_wordlist()\n",
    "        #self.read_datasets()\n",
    "        #self.read_embeds()\n",
    "        \n",
    "    def read_datasets(self):\n",
    "        self.dataset = []\n",
    "        file_strs = [\"/train.txt\", \"/dev.txt\", \"/test.txt\"]\n",
    "        for file_str in file_strs:\n",
    "            lines = map(lambda x: x.split('\\t\\t'), open(\"data/\"+self.filename + file_str).readlines())           \n",
    "            label = np.asarray(\n",
    "                map(lambda x: int(x[2])-1, lines),\n",
    "                dtype = np.int32\n",
    "            )\n",
    "            docs = map(lambda x: x[3][0:len(x[3])-1], lines) \n",
    "            docs = map(lambda x: x.split('<sssss>'), docs) \n",
    "            docs = map(lambda doc: map(lambda sentence: sentence.split(' '),doc),docs)\n",
    "            docs = map(lambda doc: map(lambda sentence: \n",
    "                                       filter(lambda wordid: wordid !=-1,\n",
    "                                              map(lambda word: self.getID(word),sentence)),doc),docs)\n",
    "            dataset = [docs, label];\n",
    "            self.dataset.append(dataset)\n",
    "            \n",
    "    def read_embeds(self):\n",
    "        f = file('data/'+self.filename+'/embinit.save', 'rb')\n",
    "        self.embeds = cPickle.load(f)\n",
    "        f.close()\n",
    "        \n",
    "    def read_wordlist(self):\n",
    "        lines = map(lambda x: x.split(), open(\"data/\" + self.filename+\"/wordlist.txt\").readlines())\n",
    "        self.size = len(lines)\n",
    "        self.voc = [(item[0][0], item[1]) for item in zip(lines, xrange(self.size))]\n",
    "        self.inv_voc = [(item[1], item[0][0]) for item in zip(lines, xrange(self.size))]\n",
    "        self.voc = dict(self.voc)\n",
    "        self.inv_voc = dict(self.inv_voc)\n",
    "\n",
    "    def getID(self, word):\n",
    "        try:\n",
    "            return self.voc[word]\n",
    "        except:\n",
    "            return -1\n",
    "    def getWord(self, id):\n",
    "        try:\n",
    "            return self.inv_voc[id]\n",
    "        except:\n",
    "            return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_reader = DataReader(\"IMDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "761"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reader.read_wordlist()\n",
    "data_reader.voc['expected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## the last one is \"UNK\"\n",
    "data_reader.read_embeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_reader.read_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset =  data_reader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "9\n",
      "11\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "## 每句话的长度包括句子两端的<sess>，不过第一句话只有一个sess\n",
    "##trainset[0]:docs , trainset[1]:labels\n",
    "##trainset[0][0]:doc trainset[0][0][0]:sentence\n",
    "print(len(trainset[0][4]))#doc\n",
    "print(len(trainset[0][0][4]))#sentence\n",
    "print(trainset[0][0][0][0])#word\n",
    "print(trainset[1][0])#label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "print(data_reader.getID('from'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excepted\n"
     ]
    }
   ],
   "source": [
    "print(data_reader.getWord(26957))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_maxlen = 20##\n",
    "sentence_maxlen = 25##\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## padding the sentences and docs to the same size\n",
    "## to construct 3d tensor size [batch_size, sentence_num_max,word_num_max]\n",
    "## At the same time, remember the length of the docs and sentence for the tensorflow dynamic rnn \n",
    "## dynamic rnn can really save your time\n",
    "def pad_doc(doc, docs_maxlen):\n",
    "    if len(doc) < docs_maxlen:\n",
    "        doc = doc + (docs_maxlen-len(doc))*[[0]]\n",
    "    else:\n",
    "        doc = doc[0:docs_maxlen]\n",
    "    return doc\n",
    "    \n",
    "def pad_sentence(sentence, sentence_maxlen):\n",
    "    if len(sentence) < sentence_maxlen:\n",
    "        sentence = sentence + (sentence_maxlen-len(sentence))*[0]\n",
    "    else:\n",
    "        sentence = sentence[0:sentence_maxlen]\n",
    "    return sentence\n",
    "    \n",
    "def pad_sentence_len(sentence_len, docs_maxlen):\n",
    "    new_sentence_len = []\n",
    "    for id in range(batch_size):\n",
    "        tmp = sentence_len[id]\n",
    "        if len(tmp) < docs_maxlen:\n",
    "            tmp = tmp + (docs_maxlen-len(tmp))*[0]\n",
    "        else:\n",
    "            tmp = tmp[0:docs_maxlen]\n",
    "        new_sentence_len.append(tmp)\n",
    "    return new_sentence_len\n",
    "    \n",
    "def genBatch(dataset, label, batch_size, docs_maxlen, sentence_maxlen,batch_id):\n",
    "    # get the sentence_num_max\n",
    "    docs = dataset[batch_size*batch_id:(batch_id+1)*batch_size]\n",
    "    label_batch = label[batch_size*batch_id:(batch_id+1)*batch_size]\n",
    "    #print(len(docs))#the first element shold be 8\n",
    "    #print(docs)\n",
    "    docs_len = np.array(map(lambda x: len(x), docs))\n",
    "    docs_len[docs_len>docs_maxlen] = docs_maxlen\n",
    "    #print(docs_len)#\n",
    "    #docs_maxlen = np.max(docs_len)\n",
    "    #print('docs maxlen is %d' %docs_maxlen)\n",
    "    # get the word_num_max\n",
    "    sentence_len = np.array(map(lambda x :map(lambda y : len(y), x), docs))\n",
    "    sentence_len = pad_sentence_len(sentence_len, docs_maxlen)\n",
    "    for id in range(batch_size):\n",
    "        tmp = np.array(sentence_len[id])\n",
    "        tmp[tmp>sentence_maxlen] = sentence_maxlen\n",
    "        sentence_len[id] = tmp\n",
    "    \n",
    "    #sentence_len[sentence_len>sentence_maxlen] = sentence_maxlen\n",
    "    #print(sentence_len)# the first element should be 13\n",
    "    #print('the sentence maxlen is %d' % sentence_maxlen)\n",
    "    ## padding the data\n",
    "    doc_batch = map(lambda x: pad_doc(x, docs_maxlen), docs)\n",
    "    #print(doc_batch[0:2])\n",
    "    doc_batch = map(lambda x:map(lambda y: pad_sentence(y, sentence_maxlen), x), doc_batch)\n",
    "    batch = [doc_batch,docs_len, sentence_len]\n",
    "    return batch, label_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch, label_batch = genBatch(trainset[0],trainset[1], batch_size, docs_maxlen, sentence_maxlen,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 20)\n"
     ]
    }
   ],
   "source": [
    "def get_sentence_array(sentence_len,batch_size,docs_maxlen):\n",
    "    sentence_len_array = np.zeros([batch_size, docs_maxlen])\n",
    "    for i in range(batch_size):\n",
    "        sentence_len_array[i,:] = sentence_len[i]\n",
    "    return sentence_len_array\n",
    "a = get_sentence_array(batch[2], 30, 20)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(batch[2][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2247\n"
     ]
    }
   ],
   "source": [
    "batch_num = len(trainset[0])//(batch_size)\n",
    "print(batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105374\n"
     ]
    }
   ],
   "source": [
    "batch_size = 30\n",
    "max_features = data_reader.embeds.shape[0]\n",
    "hidden_size = 200\n",
    "print(max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the graph functions\n",
    "# add the placeholders\n",
    "def add_placeholders():\n",
    "    input_placeholder = tf.placeholder(tf.int32, shape=[batch_size, docs_maxlen,sentence_maxlen])\n",
    "    label_placeholder = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    #seq_len_word_placeholder = tf.placeholder(tf.int32) # dynamic rnn for word lstm layer\n",
    "    #seq_len_sentence_placeholder = tf.placeholder(tf.int32) #dynamic rnn for sentence word lstm layer\n",
    "    return input_placeholder, label_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_embed_layer(vocab_size,  input_placeholder):\n",
    "    embed_size = 200\n",
    "    with tf.device('/cpu:0'), tf.variable_scope('embed'):\n",
    "        embed = tf.get_variable(name=\"Embedding\", shape=[vocab_size, embed_size])\n",
    "        inputs = tf.nn.embedding_lookup(embed, input_placeholder)\n",
    "        #inputs = tf.transpose(inputs, perm=[0,2,1])  \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## add lstm layer\n",
    "def add_rnn_model(hidden_size):\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=0.0)\n",
    "    return lstm_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## add training op\n",
    "def add_train_op(loss):\n",
    "    train_op = tf.train.AdamOptimizer(0.000001).minimize(loss)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function dynamic_rnn in module tensorflow.python.ops.rnn:\n",
      "\n",
      "dynamic_rnn(cell, inputs, sequence_length=None, initial_state=None, dtype=None, parallel_iterations=None, swap_memory=False, time_major=False, scope=None)\n",
      "    Creates a recurrent neural network specified by RNNCell `cell`.\n",
      "    \n",
      "    This function is functionally identical to the function `rnn` above, but\n",
      "    performs fully dynamic unrolling of `inputs`.\n",
      "    \n",
      "    Unlike `rnn`, the input `inputs` is not a Python list of `Tensors`, one for\n",
      "    each frame.  Instead, `inputs` may be a single `Tensor` where\n",
      "    the maximum time is either the first or second dimension (see the parameter\n",
      "    `time_major`).  Alternatively, it may be a (possibly nested) tuple of\n",
      "    Tensors, each of them having matching batch and time dimensions.\n",
      "    The corresponding output is either a single `Tensor` having the same number\n",
      "    of time steps and batch size, or a (possibly nested) tuple of such tensors,\n",
      "    matching the nested structure of `cell.output_size`.\n",
      "    \n",
      "    The parameter `sequence_length` is optional and is used to copy-through state\n",
      "    and zero-out outputs when past a batch element's sequence length. So it's more\n",
      "    for correctness than performance, unlike in rnn().\n",
      "    \n",
      "    Args:\n",
      "      cell: An instance of RNNCell.\n",
      "      inputs: The RNN inputs.\n",
      "    \n",
      "        If `time_major == False` (default), this must be a `Tensor` of shape:\n",
      "          `[batch_size, max_time, ...]`, or a nested tuple of such\n",
      "          elements.\n",
      "    \n",
      "        If `time_major == True`, this must be a `Tensor` of shape:\n",
      "          `[max_time, batch_size, ...]`, or a nested tuple of such\n",
      "          elements.\n",
      "    \n",
      "        This may also be a (possibly nested) tuple of Tensors satisfying\n",
      "        this property.  The first two dimensions must match across all the inputs,\n",
      "        but otherwise the ranks and other shape components may differ.\n",
      "        In this case, input to `cell` at each time-step will replicate the\n",
      "        structure of these tuples, except for the time dimension (from which the\n",
      "        time is taken).\n",
      "    \n",
      "        The input to `cell` at each time step will be a `Tensor` or (possibly\n",
      "        nested) tuple of Tensors each with dimensions `[batch_size, ...]`.\n",
      "      sequence_length: (optional) An int32/int64 vector sized `[batch_size]`.\n",
      "      initial_state: (optional) An initial state for the RNN.\n",
      "        If `cell.state_size` is an integer, this must be\n",
      "        a `Tensor` of appropriate type and shape `[batch_size x cell.state_size]`.\n",
      "        If `cell.state_size` is a tuple, this should be a tuple of\n",
      "        tensors having shapes `[batch_size, s] for s in cell.state_size`.\n",
      "      dtype: (optional) The data type for the initial state and expected output.\n",
      "        Required if initial_state is not provided or RNN state has a heterogeneous\n",
      "        dtype.\n",
      "      parallel_iterations: (Default: 32).  The number of iterations to run in\n",
      "        parallel.  Those operations which do not have any temporal dependency\n",
      "        and can be run in parallel, will be.  This parameter trades off\n",
      "        time for space.  Values >> 1 use more memory but take less time,\n",
      "        while smaller values use less memory but computations take longer.\n",
      "      swap_memory: Transparently swap the tensors produced in forward inference\n",
      "        but needed for back prop from GPU to CPU.  This allows training RNNs\n",
      "        which would typically not fit on a single GPU, with very minimal (or no)\n",
      "        performance penalty.\n",
      "      time_major: The shape format of the `inputs` and `outputs` Tensors.\n",
      "        If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
      "        If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
      "        Using `time_major = True` is a bit more efficient because it avoids\n",
      "        transposes at the beginning and end of the RNN calculation.  However,\n",
      "        most TensorFlow data is batch-major, so by default this function\n",
      "        accepts input and emits output in batch-major form.\n",
      "      scope: VariableScope for the created subgraph; defaults to \"RNN\".\n",
      "    \n",
      "    Returns:\n",
      "      A pair (outputs, state) where:\n",
      "    \n",
      "        outputs: The RNN output `Tensor`.\n",
      "    \n",
      "          If time_major == False (default), this will be a `Tensor` shaped:\n",
      "            `[batch_size, max_time, cell.output_size]`.\n",
      "    \n",
      "          If time_major == True, this will be a `Tensor` shaped:\n",
      "            `[max_time, batch_size, cell.output_size]`.\n",
      "    \n",
      "          Note, if `cell.output_size` is a (possibly nested) tuple of integers\n",
      "          or `TensorShape` objects, then `outputs` will be a tuple having the\n",
      "          same structure as `cell.output_size`, containing Tensors having shapes\n",
      "          corresponding to the shape data in `cell.output_size`.\n",
      "    \n",
      "        state: The final state.  If `cell.state_size` is an int, this\n",
      "          will be shaped `[batch_size, cell.state_size]`.  If it is a\n",
      "          `TensorShape`, this will be shaped `[batch_size] + cell.state_size`.\n",
      "          If it is a (possibly nested) tuple of ints or `TensorShape`, this will\n",
      "          be a tuple having the corresponding shapes.\n",
      "    \n",
      "    Raises:\n",
      "      TypeError: If `cell` is not an instance of RNNCell.\n",
      "      ValueError: If inputs is None or an empty list.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.nn.dynamic_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluation(logits, labels):\n",
    "  correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "  return tf.reduce_sum(tf.cast(correct, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reduce_mean in module tensorflow.python.ops.math_ops:\n",
      "\n",
      "reduce_mean(input_tensor, reduction_indices=None, keep_dims=False, name=None)\n",
      "    Computes the mean of elements across dimensions of a tensor.\n",
      "    \n",
      "    Reduces `input_tensor` along the dimensions given in `reduction_indices`.\n",
      "    Unless `keep_dims` is true, the rank of the tensor is reduced by 1 for each\n",
      "    entry in `reduction_indices`. If `keep_dims` is true, the reduced dimensions\n",
      "    are retained with length 1.\n",
      "    \n",
      "    If `reduction_indices` has no entries, all dimensions are reduced, and a\n",
      "    tensor with a single element is returned.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    # 'x' is [[1., 1.]\n",
      "    #         [2., 2.]]\n",
      "    tf.reduce_mean(x) ==> 1.5\n",
      "    tf.reduce_mean(x, 0) ==> [1.5, 1.5]\n",
      "    tf.reduce_mean(x, 1) ==> [1.,  2.]\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      input_tensor: The tensor to reduce. Should have numeric type.\n",
      "      reduction_indices: The dimensions to reduce. If `None` (the default),\n",
      "        reduces all dimensions.\n",
      "      keep_dims: If true, retains reduced dimensions with length 1.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      The reduced tensor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.reduce_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105374\n",
      "0 Epoch starts, Training....\n",
      "step 0 : loss : 3.159263\n",
      "step 100 : loss : 3.029322\n",
      "step 200 : loss : 2.989373\n",
      "step 300 : loss : 2.975912\n",
      "step 400 : loss : 2.962045\n",
      "step 500 : loss : 2.862700\n",
      "step 600 : loss : 2.901631\n",
      "step 700 : loss : 2.952078\n",
      "step 800 : loss : 2.902164\n",
      "step 900 : loss : 2.923768\n",
      "step 1000 : loss : 2.974546\n",
      "step 1100 : loss : 2.962385\n",
      "step 1200 : loss : 2.986768\n",
      "step 1300 : loss : 2.933807\n",
      "step 1400 : loss : 2.939800\n",
      "step 1500 : loss : 3.009450\n",
      "step 1600 : loss : 2.971812\n",
      "step 1700 : loss : 2.858559\n",
      "step 1800 : loss : 2.908821\n",
      "step 1900 : loss : 2.939851\n",
      "step 2000 : loss : 2.952921\n",
      "step 2100 : loss : 2.948024\n",
      "step 2200 : loss : 2.845860\n",
      "precision: 0.190580\n",
      "1 Epoch starts, Training....\n",
      "step 0 : loss : 3.159263\n",
      "step 100 : loss : 3.029322\n",
      "step 200 : loss : 2.989373\n",
      "step 300 : loss : 2.975912\n",
      "step 400 : loss : 2.962045\n",
      "step 500 : loss : 2.862700\n",
      "step 600 : loss : 2.901631\n",
      "step 700 : loss : 2.952078\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-3e3a8dce4400>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m#input_step = sess.run([word_inputs], feed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             loss_step,correct_num_step,w_init_state,s_init_state = sess.run([loss, correct_num,\n\u001b[0;32m---> 88\u001b[0;31m                                                                             word_state,sentence_state], feed)\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;31m#print(loss_step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wang/tensorflow_dl/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wang/tensorflow_dl/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wang/tensorflow_dl/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/wang/tensorflow_dl/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wang/tensorflow_dl/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    input_placeholder, label_placeholder = add_placeholders()\n",
    "    word_sequence_length_placeholder = tf.placeholder(tf.int32)\n",
    "    sentence_sequence_length_placeholder = tf.placeholder(tf.int32)\n",
    "    word_initial_state_placeholder = tf.placeholder(tf.float32)\n",
    "    vocab_size = max_features\n",
    "    print(vocab_size)\n",
    "    word_inputs = add_embed_layer(vocab_size, input_placeholder)\n",
    "    ##word lstm\n",
    "    cell = add_rnn_model(hidden_size)\n",
    "    word_initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    # state is the final state\n",
    "    \n",
    "    # batch_size major\n",
    "    with tf.variable_scope('word_rnn'):\n",
    "        sentence_inputs = []\n",
    "        for sentence_id in range(docs_maxlen):\n",
    "            if sentence_id > 0:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "            word_output, word_state = tf.nn.dynamic_rnn(cell, tf.squeeze(word_inputs[:,sentence_id,:,:]), \n",
    "                                                        initial_state = word_initial_state,\n",
    "                                                        sequence_length=word_sequence_length_placeholder[:, sentence_id])\n",
    "            \n",
    "            sentence_inputs.append(tf.reduce_mean(word_output, 1))\n",
    "        \n",
    "    \n",
    "    ## sentence lstm\n",
    "    ## construct the sentence inputs\n",
    "    \n",
    "    sentence_inputs_tensor = tf.pack(sentence_inputs, axis=0)\n",
    "    \n",
    "    cell = add_rnn_model(hidden_size)\n",
    "    sentence_initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    sentence_outputs, sentence_state = tf.nn.dynamic_rnn(cell, sentence_inputs_tensor,\n",
    "                                                         initial_state=sentence_initial_state,\n",
    "                                                         time_major=True, \n",
    "                                                         sequence_length=sentence_sequence_length_placeholder)\n",
    "    ##time major\n",
    "    \n",
    "    doc_output = tf.reduce_mean(sentence_outputs, 0)\n",
    "    tf.expand_dims(doc_output,0)\n",
    "    ##Fully Connected Layer\n",
    "    W_fc = tf.get_variable('Weights_fc', shape=[hidden_size, hidden_size])\n",
    "    B_fc = tf.get_variable('bias_fc', shape=[hidden_size])\n",
    "    h_fc = tf.nn.relu(tf.matmul(doc_output,W_fc)+B_fc)\n",
    "    #add projection layer\n",
    "    W = tf.get_variable('Weights', shape=[hidden_size, class_num])\n",
    "    b = tf.get_variable('Bias', shape = [class_num])\n",
    "    \n",
    "    y_pred = tf.matmul(h_fc, W) + b\n",
    "    \n",
    "    #y_pred_sigmoid = tf.sigmoid(y_pred)\n",
    "    \n",
    "    correct_num = evaluation(y_pred, label_placeholder)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(y_pred, label_placeholder)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    train_op = add_train_op(loss)\n",
    "    \n",
    "    #state_step = initial_state\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    max_epochs = 2\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        w_init_state = sess.run([word_initial_state])\n",
    "        s_init_state = sess.run([sentence_initial_state])\n",
    "        print('%d Epoch starts, Training....' %(epoch))\n",
    "        mean_loss = []\n",
    "        total_correct_num = 0\n",
    "        for step in range(batch_num):\n",
    "            # generate the data feed dict\n",
    "            \n",
    "            #batch = [doc_batch,docs_len, sentence_len]\n",
    "            batch, label_batch = genBatch(trainset[0],trainset[1], batch_size, docs_maxlen, sentence_maxlen,step)\n",
    "            input_batch = np.array(batch[0])\n",
    "            word_sq_len_batch = get_sentence_array(batch[2],batch_size,docs_maxlen)\n",
    "            #print(word_sq_len_batch.shape)\n",
    "             \n",
    "            sentence_len_batch = np.array(batch[1])\n",
    "            #print(sentece_len_batch.shape)\n",
    "            #print(word_sq_len_batch)\n",
    "            feed = {input_placeholder:input_batch, word_initial_state:w_init_state,\n",
    "                   label_placeholder:label_batch, sentence_initial_state:s_init_state,\n",
    "                   word_sequence_length_placeholder:word_sq_len_batch,\n",
    "                   sentence_sequence_length_placeholder:sentence_len_batch}\n",
    "            #input_step = sess.run([word_inputs], feed)\n",
    "            loss_step,correct_num_step,w_init_state,s_init_state = sess.run([loss, correct_num,\n",
    "                                                                            word_state,sentence_state], feed)\n",
    "            #print(loss_step)\n",
    "        \n",
    "            mean_loss.append(loss_step)\n",
    "            total_correct_num += correct_num_step\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                print('step %d : loss : %f' %(step, np.mean(mean_loss)))\n",
    "                mean_loss = []\n",
    "            #do_evaluation(sess, X_test, y_test)\n",
    "            \n",
    "        \n",
    "        print('precision: %f' %(total_correct_num/(batch_size*batch_num)))\n",
    "        #print('Testing....')\n",
    "        #do_evaluation(sess, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.asarray([[1,2],[3,4]])\n",
    "print(a)\n",
    "a.dimshuffle(0,'x') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(doc_output_step))\n",
    "print(doc_output_step[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(y_pred_sigmoid_step.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(loss_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
