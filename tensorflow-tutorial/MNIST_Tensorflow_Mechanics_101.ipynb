{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import the lib tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASS = 10\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE*IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_boolean(\"fake_data\", False, 'If true, use fake data for unit testing')\n",
    "flags.DEFINE_string(\"train_dir\", 'data', 'Directory to put the data')\n",
    "flags.DEFINE_integer(\"batch_size\", 100, \"batch size , must be divided evenly into the datasize\")\n",
    "flags.DEFINE_integer('hidden1_units', 128, 'number of units in hidden layer 1')\n",
    "flags.DEFINE_integer('hidden2_units', 32, 'number of units in hidden layer 2')\n",
    "flags.DEFINE_integer('max_train_steps', 200, 'number of the max training steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "data_sets = input_data.read_data_sets(FLAGS.train_dir, FLAGS.fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 6 1 8 1 0 9 8 0 3 1 2 7 0 2 9 6 0]\n"
     ]
    }
   ],
   "source": [
    "print(data_sets.train.labels[1:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference stage\n",
    "Build the model:\n",
    "\n",
    "args:\n",
    "\n",
    "images : Image placeholder, \n",
    "\n",
    "hidden1_units : size of the first hidden layer\n",
    "\n",
    "hidden2_units : size of the second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(images, hidden1_units, hidden2_units):\n",
    "    #hidden1\n",
    "    with tf.name_scope(\"hidden1\"):\n",
    "        weights = tf.Variable(tf.truncated_normal([IMAGE_PIXELS, hidden1_units], \n",
    "                                                  stddev=1.0/math.sqrt(float(IMAGE_PIXELS))),\n",
    "                             name=\"weights\")\n",
    "        biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                            name=\"biases\")\n",
    "        hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "    \n",
    "    #hidden2\n",
    "    with tf.name_scope(\"hidden2\"):\n",
    "        weights = tf.Variable(tf.truncated_normal([hidden1_units, hidden2_units], \n",
    "                                                  stddev=1.0/math.sqrt(float(hidden1_units))),\n",
    "                             name=\"weights\")\n",
    "        biases = tf.Variable(tf.zeros([hidden2_units]),\n",
    "                            name=\"biases\")\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "    #Linear softmax layer\n",
    "    with tf.name_scope(\"softmax_linear\"):\n",
    "        weights = tf.Variable(tf.truncated_normal([hidden2_units, NUM_CLASS], \n",
    "                                                  stddev=1.0/math.sqrt(hidden2_units)),\n",
    "                             name=\"weights\")\n",
    "        biases = tf.Variable(tf.zeros([NUM_CLASS]),\n",
    "                            name=\"biases\")\n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_cal(logits, labels): \n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name=\"xentropy\")\n",
    "    loss = tf.reduce_mean(cross_entropy, name=\"xentropy_mean\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluation(logits, labels):\n",
    "  correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "  return tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "\n",
    "def do_eval(sess, eval_correct, data_sets, image_placeholder, label_placeholder):\n",
    "    true_count = 0\n",
    "    steps_per_epoch = data_sets.num_examples // FLAGS.batch_size\n",
    "    num_examples = steps_per_epoch * FLAGS.batch_size\n",
    "    for step in xrange(steps_per_epoch):\n",
    "        batch = data_sets.next_batch(FLAGS.batch_size, FLAGS.fake_data)\n",
    "        true_count += sess.run(eval_correct, feed_dict={image_placeholder:batch[0], label_placeholder:batch[1]})\n",
    "    precision = true_count / num_examples\n",
    "    print('Precision: %f' % (precision))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xentropy_mean\n",
      "Precision: 0.477500\n"
     ]
    }
   ],
   "source": [
    "# tell tensorflow that the model will build into the default Graph\n",
    "with tf.Graph().as_default():\n",
    "    # generate the placeholder for the imgaes and labels\n",
    "    image_placeholder = tf.placeholder(tf.float32, shape=[FLAGS.batch_size, IMAGE_PIXELS])\n",
    "    label_placeholder = tf.placeholder(tf.int32, shape=[FLAGS.batch_size])\n",
    "    #build the graph from the inference model\n",
    "    logits = inference(image_placeholder, FLAGS.hidden1_units, FLAGS.hidden2_units)\n",
    "    loss = loss_cal(logits, label_placeholder)\n",
    "    #\n",
    "    tf.scalar_summary(loss.op.name, loss)\n",
    "    print (loss.op.name)\n",
    "    ##build the summary \n",
    "    summary_op = tf.merge_all_summaries()\n",
    "    #\n",
    "    sess = tf.Session()\n",
    "    summary_writer = tf.train.SummaryWriter(FLAGS.train_dir, sess.graph)\n",
    "    \n",
    "    \n",
    "    eval_correct = evaluation(logits, label_placeholder)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "    sess.run(tf.initialize_all_variables())\n",
    "   \n",
    "    for step in range(FLAGS.max_train_steps):\n",
    "        batch = data_sets.train.next_batch(FLAGS.batch_size, FLAGS.fake_data)\n",
    "        _ = sess.run([train_op,summary_op], feed_dict={image_placeholder: batch[0], label_placeholder: batch[1]})\n",
    "        if step % 10 == 0:\n",
    "            summary_str = sess.run(summary_op, feed_dict={image_placeholder: batch[0], label_placeholder: batch[1]})\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "    do_eval(sess, eval_correct, data_sets.test, image_placeholder, label_placeholder )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
