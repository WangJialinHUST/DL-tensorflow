{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Some Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Generate Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the wordvectors and the words\n",
    "def load_word_vec():\n",
    "    word2vec = np.loadtxt('data/ner/wordVectors.txt')\n",
    "    with open('data/ner/vocab.txt') as fd:\n",
    "       words = [line.strip() for line in fd]\n",
    "    words_dict = dict(enumerate(words))\n",
    "    return words_dict, word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the style of words_dict is : {index :word}\n",
    "# after inverse, get {word : index}\n",
    "def invert_dict(dictionary):\n",
    "    inv_dict = {}\n",
    "    for key, value in dictionary.items():\n",
    "        inv_dict.setdefault(value, key)\n",
    "    return inv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## read the documents, \n",
    "## generate the dataset List[word, class]\n",
    "def generate_dataset(filename):\n",
    "    #load the set\n",
    "    docs = []\n",
    "    cur_line = []\n",
    "    \n",
    "    with open(filename) as fd:\n",
    "        for line in fd:\n",
    "            #begin of th doc \n",
    "            if re.match(r\"-DOCSTART-.+\", line) or (len(line.strip())==0):\n",
    "                if(len(line.strip())==0):\n",
    "                    #[] denote the begining or end of the sentence \n",
    "                    cur_line = ['<s>']\n",
    "                    docs.append(cur_line)\n",
    "            else:\n",
    "                cur_line = line.strip().split('\\t', 1)\n",
    "                docs.append(cur_line)\n",
    "                #print(docs[0])\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## generate word context (windows default size = 3) from docs\n",
    "## note: string '<s>' is just a notation for empty of the word\n",
    "## example : \n",
    "def docs_to_windows(docs, word_dict, tag_dict, window_size = 3):\n",
    "    #from words to indices\n",
    "    #at the begin and the end add the paddings\n",
    "    \n",
    "    words = []\n",
    "    tags = []\n",
    "    for index in range(len(docs)):\n",
    "        if docs[index] == ['<s>']:\n",
    "            continue;\n",
    "        else:\n",
    "            item = list([docs[index-1][0], docs[index][0],docs[index+1][0]])\n",
    "            words.append(item)\n",
    "            tags.append(docs[index][1])\n",
    "    return words, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## lookup the inv_words_dict, \n",
    "## change the word to index \n",
    "def word_to_index(words, inv_words_dict):\n",
    "    indices = []\n",
    "    for item in words:\n",
    "        item_indices = []\n",
    "        for word in item:\n",
    "            if inv_words_dict.has_key(word.lower()):\n",
    "                item_indices.append(inv_words_dict[word.lower()])\n",
    "            else:\n",
    "                #for word not in the vacabulary ,use unknown word'UUUNKKK' denote\n",
    "                word = 'UUUNKKK'\n",
    "                item_indices.append(inv_words_dict[word])\n",
    "        \n",
    "        indices.append(item_indices)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## lookup the inv_tag_dict\n",
    "## change the tag to index\n",
    "def tag_to_index(tags, inv_tag_dict):\n",
    "    return [inv_tag_dict[tag] for tag in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## shuffle the data \n",
    "## shuffle the words and the tags at the same time\n",
    "def shuffle_data(matrix_A, matrix_B):\n",
    "    indices = np.random.permutation(len(matrix_A))\n",
    "    return matrix_A[indices], matrix_B[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## add the placeholders\n",
    "def add_placeholders():\n",
    "    #input\n",
    "    input_placeholder = tf.placeholder(dtype=tf.int32, shape=[None, 3])\n",
    "    #labels\n",
    "    label_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, 5])\n",
    "    return input_placeholder, label_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create the feed dict\n",
    "def create_feed_dict(input_placeholder, input_batch, label_placeholder, label_batch):\n",
    "    feed_dict = {input_placeholder: input_batch,\n",
    "                label_placeholder:label_batch}\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluation(y_pred, labels):\n",
    "    label_right = tf.argmax(labels, dimension=1)\n",
    "    label_pred = tf.argmax(y_pred, dimension=1)\n",
    "    correct_pred_num =  tf.reduce_sum(tf.cast(tf.equal(label_right, label_pred), tf.int32))\n",
    "    return correct_pred_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_eval(sess, eval_correct, words_indices,\n",
    "            batch_size, input_placeholder, label_placeholder,tags_matrix):\n",
    "    true_count = 0\n",
    "    steps_per_epoch = len(tags_test) // batch_size\n",
    "    \n",
    "    num_examples = steps_per_epoch * batch_size\n",
    "    valid_loss = []\n",
    "    for step in range(steps_per_epoch):\n",
    "        input_batch = words_indices[step*batch_size: (step+1)*batch_size, :]\n",
    "        label_batch = tags_matrix[step*batch_size: (step+1)*batch_size, :]\n",
    "        feed_dict = create_feed_dict(input_placeholder, input_batch, label_placeholder, label_batch)\n",
    "        count_step,loss_step  = sess.run([eval_correct, loss], feed_dict)\n",
    "        true_count += count_step\n",
    "        valid_loss.append(loss_step)\n",
    "    mean_loss = np.mean(valid_loss)\n",
    "    accuracy = true_count / num_examples\n",
    "    print('Validation Stage: Loss : %f, Validation Accuracy : %f' %(mean_loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('batch_size', 64, 'the size of the batch')\n",
    "flags.DEFINE_integer('max_epoch', 1, 'the max times to rerun the training progress')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_dict, word2vec = load_word_vec()\n",
    "word2vec = word2vec.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inv_words_dict = invert_dict(words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': 1, 'MISC': 2, 'PER': 4, 'O': 0, 'ORG': 3}\n"
     ]
    }
   ],
   "source": [
    "tagnames = ['O', 'LOC', 'MISC', 'ORG', 'PER']\n",
    "tag_dict = dict(enumerate(tagnames))\n",
    "inv_tag_dict = invert_dict(tag_dict)\n",
    "print(inv_tag_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate the training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = generate_dataset('data/ner/train')\n",
    "words, tags = docs_to_windows(docs, words_dict, tag_dict)\n",
    "words_indices = np.array(word_to_index(words, inv_words_dict))\n",
    "tags_indices = np.array(tag_to_index(tags, inv_tag_dict))\n",
    "tags_matrix = np.array(np.zeros([len(tags_indices), 5]))\n",
    "for i in range(len(tags_indices)):\n",
    "    index = tags_indices[i]\n",
    "    tags_matrix[i, index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate the validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_test = generate_dataset('data/ner/dev')\n",
    "words_test, tags_test = docs_to_windows(docs_test, words_dict, tag_dict)\n",
    "words_indices_test = np.array(word_to_index(words_test, inv_words_dict))\n",
    "tags_indices_test = np.array(tag_to_index(tags_test, inv_tag_dict))\n",
    "tags_matrix_test = np.array(np.zeros([len(tags_indices_test), 5]))\n",
    "for i in range(len(tags_indices_test)):\n",
    "    index = tags_indices_test[i]\n",
    "    tags_matrix_test[i, index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_step = len(tags_indices) // FLAGS.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### a simple majority guess classifier\n",
    "### if we guess the entity is 'O', how much probability that we will get right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.832811939829\n",
      "0.832502628402\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(tags)):\n",
    "    if tags[i] == 'O':\n",
    "        count += 1\n",
    "print(count/len(tags))\n",
    "\n",
    "count = 0\n",
    "for i in range(len(tags_test)):\n",
    "    if tags_test[i] == 'O':\n",
    "        count += 1\n",
    "print(count/len(tags_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 , trainging\n",
      "step 0 / 3181: loss 1.641708\n",
      "step 100 / 3181: loss 0.336718\n",
      "step 200 / 3181: loss 0.636345\n",
      "step 300 / 3181: loss 0.293760\n",
      "step 400 / 3181: loss 0.513436\n",
      "step 500 / 3181: loss 0.445194\n",
      "step 600 / 3181: loss 0.313875\n",
      "step 700 / 3181: loss 0.416534\n",
      "step 800 / 3181: loss 0.343436\n",
      "step 900 / 3181: loss 0.359057\n",
      "step 1000 / 3181: loss 0.283767\n",
      "step 1100 / 3181: loss 0.231637\n",
      "step 1200 / 3181: loss 0.313768\n",
      "step 1300 / 3181: loss 0.291815\n",
      "step 1400 / 3181: loss 0.287816\n",
      "step 1500 / 3181: loss 0.248984\n",
      "step 1600 / 3181: loss 0.255572\n",
      "step 1700 / 3181: loss 0.253633\n",
      "step 1800 / 3181: loss 0.222316\n",
      "step 1900 / 3181: loss 0.318004\n",
      "step 2000 / 3181: loss 0.301382\n",
      "step 2100 / 3181: loss 0.160079\n",
      "step 2200 / 3181: loss 0.261188\n",
      "step 2300 / 3181: loss 0.133320\n",
      "step 2400 / 3181: loss 0.338615\n",
      "step 2500 / 3181: loss 0.360968\n",
      "step 2600 / 3181: loss 0.212219\n",
      "step 2700 / 3181: loss 0.358713\n",
      "step 2800 / 3181: loss 0.225175\n",
      "step 2900 / 3181: loss 0.183463\n",
      "step 3000 / 3181: loss 0.385651\n",
      "step 3100 / 3181: loss 0.126905\n",
      "epoch 0  Training Accuracy : 0.923147 \n",
      " \n",
      "\n",
      "epoch 0, Validating\n",
      "Validation Stage: Loss : 0.242173, Validation Accuracy : 0.943364\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.device('/cpu:0'):\n",
    "    #add placeholders\n",
    "    input_placeholder, label_placeholder = add_placeholders()\n",
    "    #add the embed layer\n",
    "    with  tf.device('/cpu:0'):\n",
    "        embedding = tf.get_variable('Embedding', [len(word2vec), 50]) \n",
    "        embeds = tf.nn.embedding_lookup(embedding, input_placeholder)\n",
    "        embeds = tf.reshape(embeds, [-1, 150])\n",
    "    #for the hidden unit\n",
    "    W = tf.Variable(tf.random_uniform([150, 100], \n",
    "                                      minval=-np.sqrt(6.0/(150+100)), maxval = np.sqrt(6.0/(150+100))))\n",
    "    b1 = tf.Variable(tf.zeros([100]))\n",
    "    h = tf.nn.tanh(tf.matmul(embeds, W) + b1)\n",
    "    U = tf.Variable(tf.random_uniform([100, 5], \n",
    "                                      minval=-np.sqrt(6.0/(100+5)), maxval = np.sqrt(6.0/(100+5))))\n",
    "    b2 = tf.Variable(tf.zeros([5]))\n",
    "    y = tf.matmul(h, U) + b2\n",
    "    y = tf.nn.dropout(y, 0.9)\n",
    "    pred = tf.nn.softmax(y)\n",
    "    #\n",
    "    eval_correct = evaluation(pred, label_placeholder)\n",
    "    ##regulurization\n",
    "    #L2 = tf.reduce_sum(tf.square(W)) + tf.reduce_sum(tf.square(U))\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(label_placeholder*tf.log(tf.clip_by_value(pred,1e-7,1.0)), \n",
    "                                         reduction_indices=[1]))+ 0.5*0.001*tf.nn.l2_loss(W)+ 0.5*0.001*tf.nn.l2_loss(U)\n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, label_placeholder)) + 0.5*0.001*tf.nn.l2_loss(W)+ 0.5*0.001*tf.nn.l2_loss(U)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    for epoch in range(FLAGS.max_epoch):\n",
    "        \n",
    "        #shuffle the data\n",
    "        words_indices, tags_matrix = shuffle_data(words_indices, tags_matrix)\n",
    "        words_indices_test, tags_matrix_test = shuffle_data(words_indices_test, tags_matrix_test)\n",
    "        correct_num = []\n",
    "        print('epoch %d , Trainging' % (epoch))\n",
    "        for step in range(max_step):\n",
    "            input_batch = words_indices[step*FLAGS.batch_size: (step+1)*FLAGS.batch_size, :]\n",
    "            label_batch = tags_matrix[step*FLAGS.batch_size: (step+1)*FLAGS.batch_size, :]\n",
    "            \n",
    "            ##shuffle the data\n",
    "            feed_dict = create_feed_dict(input_placeholder, input_batch, label_placeholder, label_batch)\n",
    "            _, loss_step, correct_step = sess.run([train_op, loss, eval_correct], feed_dict)\n",
    "            correct_num.append(correct_step)\n",
    "            if step % 100 == 0:\n",
    "                print('step %d / %d: loss %f' %(step,  max_step,loss_step))\n",
    "        print('epoch %d  Training Accuracy : %f \\n \\n' % (epoch, np.sum(correct_num)/ (max_step*FLAGS.batch_size)))\n",
    "        print('epoch %d, Validating' % (epoch))\n",
    "        do_eval(sess, eval_correct,  words_indices_test,\n",
    "                FLAGS.batch_size, input_placeholder, label_placeholder, tags_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time:  1236.321323 seconds\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
