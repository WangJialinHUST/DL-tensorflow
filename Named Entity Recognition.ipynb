{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load vocabulary and wordvectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_word_vec():\n",
    "    word2vec = np.loadtxt('data/ner/wordVectors.txt')\n",
    "    with open('data/ner/vocab.txt') as fd:\n",
    "       words = [line.strip() for line in fd]\n",
    "    words_dict = dict(enumerate(words))\n",
    "    return words_dict, word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def invert_dict(dictionary):\n",
    "    inv_dict = {}\n",
    "    for key, value in dictionary.items():\n",
    "        inv_dict.setdefault(value, key)\n",
    "    return inv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_dict, word2vec = load_word_vec()\n",
    "word2vec = word2vec.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inv_words_dict = invert_dict(words_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100232, 50)\n"
     ]
    }
   ],
   "source": [
    "print(word2vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load and Generate the train_set satisfy the requirement"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_docs example \n",
    "\n",
    "-DOCSTART-\tO\n",
    "\n",
    "EU\tORG\n",
    "rejects\tO\n",
    "German\tMISC\n",
    "call\tO\n",
    "to\tO\n",
    "boycott\tO\n",
    "British\tMISC\n",
    "lamb\tO\n",
    ".\tO\n",
    "\n",
    "Peter\tPER\n",
    "Blackburn\tPER\n",
    "\n",
    "BRUSSELS\tLOC\n",
    "1996-08-22\tO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_dataset(filename):\n",
    "    #load the set\n",
    "    docs = []\n",
    "    cur_line = []\n",
    "    \n",
    "    with open(filename) as fd:\n",
    "        for line in fd:\n",
    "            #begin of th doc \n",
    "            if re.match(r\"-DOCSTART-.+\", line) or (len(line.strip())==0):\n",
    "                if(len(line.strip())==0):\n",
    "                    #[] denote the begining or end of the sentence \n",
    "                    cur_line = ['<s>']\n",
    "                    docs.append(cur_line)\n",
    "            else:\n",
    "                cur_line = line.strip().split('\\t', 1)\n",
    "                docs.append(cur_line)\n",
    "                #print(docs[0])\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate windows(default size = 3) from docs\n",
    "def docs_to_windows(docs, word_dict, tag_dict, window_size = 3):\n",
    "    #from words to indices\n",
    "    #at the begin and the end add the paddings\n",
    "    \n",
    "    words = []\n",
    "    tags = []\n",
    "    for index in range(len(docs)):\n",
    "        if docs[index] == ['<s>']:\n",
    "            continue;\n",
    "        else:\n",
    "            item = list([docs[index-1][0], docs[index][0],docs[index+1][0]])\n",
    "            words.append(item)\n",
    "            tags.append(docs[index][1])\n",
    "    return words, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_index(words, inv_words_dict):\n",
    "    indices = []\n",
    "    for item in words:\n",
    "        item_indices = []\n",
    "        for word in item:\n",
    "            if inv_words_dict.has_key(word.lower()):\n",
    "                item_indices.append(inv_words_dict[word.lower()])\n",
    "            else:\n",
    "                #for word not in the vacabulary ,use unknown word'UUUNKKK' denote\n",
    "                word = 'UUUNKKK'\n",
    "                item_indices.append(inv_words_dict[word])\n",
    "        \n",
    "        indices.append(item_indices)\n",
    "    return indices\n",
    "\n",
    "def tag_to_index(tags, inv_tag_dict):\n",
    "    return [inv_tag_dict[tag] for tag in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': 1, 'MISC': 2, 'PER': 4, 'O': 0, 'ORG': 3}\n"
     ]
    }
   ],
   "source": [
    "tagnames = ['O', 'LOC', 'MISC', 'ORG', 'PER']\n",
    "tag_dict = dict(enumerate(tagnames))\n",
    "inv_tag_dict = invert_dict(tag_dict)\n",
    "print(inv_tag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = generate_dataset('data/ner/train')\n",
    "words, tags = docs_to_windows(docs, words_dict, tag_dict)\n",
    "words_indices = np.array(word_to_index(words, inv_words_dict))\n",
    "tags_indices = np.array(tag_to_index(tags, inv_tag_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags_matrix = np.array(np.zeros([len(tags_indices), 5]))\n",
    "for i in range(len(tags_indices)):\n",
    "    index = tags_indices[i]\n",
    "    tags_matrix[i, index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function add_to_collection in module tensorflow.python.framework.ops:\n",
      "\n",
      "add_to_collection(name, value)\n",
      "    Wrapper for `Graph.add_to_collection()` using the default graph.\n",
      "    \n",
      "    See [`Graph.add_to_collection()`](../../api_docs/python/framework.md#Graph.add_to_collection)\n",
      "    for more details.\n",
      "    \n",
      "    Args:\n",
      "      name: The key for the collection. For example, the `GraphKeys` class\n",
      "        contains many standard names for collections.\n",
      "      value: The value to add to the collection.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.add_to_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs_test = generate_dataset('data/ner/dev')\n",
    "words_test, tags_test = docs_to_windows(docs_test, words_dict, tag_dict)\n",
    "words_indices_test = np.array(word_to_index(words_test, inv_words_dict))\n",
    "tags_indices_test = np.array(tag_to_index(tags_test, inv_tag_dict))\n",
    "tags_matrix_test = np.array(np.zeros([len(tags_indices_test), 5]))\n",
    "for i in range(len(tags_indices_test)):\n",
    "    index = tags_indices_test[i]\n",
    "    tags_matrix_test[i, index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51362, 5)\n"
     ]
    }
   ],
   "source": [
    "print(tags_matrix_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_data(matrix_A, matrix_B):\n",
    "    indices = np.random.permutation(len(matrix_A))\n",
    "    return matrix_A[indices], matrix_B[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   17   445  3510]\n",
      " [  445  3510     9]\n",
      " [ 3510     9  7037]\n",
      " [    9  7037     9]\n",
      " [ 7037     9 26237]\n",
      " [    9 26237   192]\n",
      " [26237   192  5288]\n",
      " [  192  5288   127]\n",
      " [ 5288   127  3179]\n",
      " [  127  3179  2544]]\n",
      "[[ 0.  0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.]]\n",
      "(array([[  192,  5288,   127],\n",
      "       [ 7037,     9, 26237],\n",
      "       [   17,   445,  3510],\n",
      "       [ 3510,     9,  7037],\n",
      "       [  445,  3510,     9],\n",
      "       [  127,  3179,  2544],\n",
      "       [    9,  7037,     9],\n",
      "       [ 5288,   127,  3179],\n",
      "       [26237,   192,  5288],\n",
      "       [    9, 26237,   192]]), array([[ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.]]))\n"
     ]
    }
   ],
   "source": [
    "A = words_indices[22:32]\n",
    "B = tags_matrix[22:32]\n",
    "print(A)\n",
    "print(B)\n",
    "print(shuffle_data(A,B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function permutation:\n",
      "\n",
      "permutation(...)\n",
      "    permutation(x)\n",
      "    \n",
      "    Randomly permute a sequence, or return a permuted range.\n",
      "    \n",
      "    If `x` is a multi-dimensional array, it is only shuffled along its\n",
      "    first index.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    x : int or array_like\n",
      "        If `x` is an integer, randomly permute ``np.arange(x)``.\n",
      "        If `x` is an array, make a copy and shuffle the elements\n",
      "        randomly.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    out : ndarray\n",
      "        Permuted sequence or array range.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> np.random.permutation(10)\n",
      "    array([1, 7, 4, 3, 0, 9, 2, 5, 8, 6])\n",
      "    \n",
      "    >>> np.random.permutation([1, 4, 9, 12, 15])\n",
      "    array([15,  1,  9,  4, 12])\n",
      "    \n",
      "    >>> arr = np.arange(9).reshape((3, 3))\n",
      "    >>> np.random.permutation(arr)\n",
      "    array([[6, 7, 8],\n",
      "           [0, 1, 2],\n",
      "           [3, 4, 5]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.random.permutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.832811939829\n",
      "0.832502628402\n",
      "51362\n",
      "203621\n",
      "['O', 'O', 'ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(tags)):\n",
    "    if tags[i] == 'O':\n",
    "        count += 1\n",
    "print(count/len(tags))\n",
    "\n",
    "count = 0\n",
    "for i in range(len(tags_test)):\n",
    "    if tags_test[i] == 'O':\n",
    "        count += 1\n",
    "print(count/len(tags_test))\n",
    "\n",
    "print(len(tags_test))\n",
    "print(len(tags))\n",
    "print(tags_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint(docs[1][0])\\nprint(words_indices[0:23])\\nfor i in range(words_indices.shape[0]):\\n    if words_indices[i, 2] == 30:\\n        #print('1')\\n        words_indices[i, 2] = 31\\nfor i in range(words_indices_test.shape[0]):\\n    if words_indices_test[i, 2] == 30:\\n        words_indices_test[i, 2] = 31\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(docs[1][0])\n",
    "print(words_indices[0:23])\n",
    "for i in range(words_indices.shape[0]):\n",
    "    if words_indices[i, 2] == 30:\n",
    "        #print('1')\n",
    "        words_indices[i, 2] = 31\n",
    "for i in range(words_indices_test.shape[0]):\n",
    "    if words_indices_test[i, 2] == 30:\n",
    "        words_indices_test[i, 2] = 31\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(words_indices.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_step = len(tags_indices) // batch_size\n",
    "max_epoch = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_placeholders():\n",
    "    #input\n",
    "    input_placeholder = tf.placeholder(dtype=tf.int32, shape=[None, 3])\n",
    "    #labels\n",
    "    label_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, 5])\n",
    "    return input_placeholder, label_placeholder\n",
    "\n",
    "def create_feed_dict(input_placeholder, input_batch, label_placeholder, label_batch):\n",
    "    feed_dict = {input_placeholder: input_batch,\n",
    "                label_placeholder:label_batch}\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate the batch \n",
    "def indices_to_vec(words_indices, word2vec):\n",
    "    vec = np.ndarray([batch_size, 150])\n",
    "    for i in range(batch_size):\n",
    "        #print(array(words_indices[i]))\n",
    "        result = word2vec[np.array(words_indices[i]), :]\n",
    "        result = np.reshape(result,[150])\n",
    "        vec[i, :] = result\n",
    "        #print(result.shape)\n",
    "    return vec\n",
    "\n",
    "def add_embed_layer(word2vec, input_placeholder):\n",
    "    return tf.nn.embedding_lookup(word2vec, input_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluation(y_pred, labels):\n",
    "    label_right = tf.argmax(labels, dimension=1)\n",
    "    label_pred = tf.argmax(y_pred, dimension=1)\n",
    "    correct_pred_num =  tf.reduce_sum(tf.cast(tf.equal(label_right, label_pred), tf.int32))\n",
    "    return correct_pred_num\n",
    "\n",
    "def do_eval(sess, eval_correct, words_indices,\n",
    "            batch_size, input_placeholder, label_placeholder,tags_matrix):\n",
    "    true_count = 0\n",
    "    steps_per_epoch = len(tags_test) // batch_size\n",
    "    \n",
    "    num_examples = steps_per_epoch * batch_size\n",
    "    print(num_examples)\n",
    "    test_loss = []\n",
    "    for step in range(steps_per_epoch):\n",
    "        input_batch = words_indices[step*batch_size: (step+1)*batch_size, :]\n",
    "        label_batch = tags_matrix[step*batch_size: (step+1)*batch_size, :]\n",
    "        feed_dict = create_feed_dict(input_placeholder, input_batch, label_placeholder, label_batch)\n",
    "        tmp_count,tmp_loss  = sess.run([eval_correct, loss], feed_dict)\n",
    "        true_count += tmp_count\n",
    "        test_loss.append(tmp_loss)\n",
    "    print(true_count)\n",
    "    mean_loss = np.mean(test_loss)\n",
    "    precision = true_count / num_examples\n",
    "    print('Validation Loss: %f' %(mean_loss))\n",
    "    print('Validation Precision: %f' % (precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6122\n",
      "0.436686\n",
      "0.324668\n",
      "0.458925\n",
      "0.403082\n",
      "0.200905\n",
      "0.16913\n",
      "0.22696\n",
      "0.073411\n",
      "0.297769\n",
      "0.307932\n",
      "0.215146\n",
      "0.181093\n",
      "0.163965\n",
      "0.201217\n",
      "0.151164\n",
      "0.9259765011\n",
      "51328\n",
      "48428\n",
      "Validation Loss: 0.224921\n",
      "Validation Precision: 0.943501\n",
      "0.184523\n",
      "0.0617676\n",
      "0.185237\n",
      "0.0957405\n",
      "0.104207\n",
      "0.114061\n",
      "0.221457\n",
      "0.100315\n",
      "0.100898\n",
      "0.126498\n",
      "0.113688\n",
      "0.134258\n",
      "0.134043\n",
      "0.0410077\n",
      "0.136602\n",
      "0.0914294\n",
      "0.963214201509\n",
      "51328\n",
      "48439\n",
      "Validation Loss: 0.223082\n",
      "Validation Precision: 0.943715\n",
      "0.182362\n",
      "0.0411393\n",
      "0.0780863\n",
      "0.226395\n",
      "0.0837828\n",
      "0.230036\n",
      "0.134901\n",
      "0.136362\n",
      "0.0657355\n",
      "0.16822\n",
      "0.382148\n",
      "0.137836\n",
      "0.180086\n",
      "0.220435\n",
      "0.188398\n",
      "0.114687\n",
      "0.968587904747\n",
      "51328\n",
      "48397\n",
      "Validation Loss: 0.229408\n",
      "Validation Precision: 0.942897\n",
      "0.123968\n",
      "0.0632047\n",
      "0.29622\n",
      "0.0472185\n",
      "0.312508\n",
      "0.0493164\n",
      "0.129145\n",
      "0.0993374\n",
      "0.0911122\n",
      "0.214584\n",
      "0.123246\n",
      "0.0416243\n",
      "0.130712\n",
      "0.205632\n",
      "0.118174\n",
      "0.0498126\n",
      "0.970960389815\n",
      "51328\n",
      "48467\n",
      "Validation Loss: 0.230543\n",
      "Validation Precision: 0.944260\n",
      "0.167575\n",
      "0.104121\n",
      "0.184485\n",
      "0.119779\n",
      "0.135298\n",
      "0.193462\n",
      "0.118292\n",
      "0.0761743\n",
      "0.109811\n",
      "0.0877448\n",
      "0.0951058\n",
      "0.0463479\n",
      "0.0934122\n",
      "0.0563546\n",
      "0.0749786\n",
      "0.0665265\n",
      "0.972041024835\n",
      "51328\n",
      "48436\n",
      "Validation Loss: 0.235579\n",
      "Validation Precision: 0.943656\n",
      "0.0692648\n",
      "0.0770501\n",
      "0.105688\n",
      "0.0762895\n",
      "0.0459978\n",
      "0.0602959\n",
      "0.252731\n",
      "0.0421334\n",
      "0.159066\n",
      "0.194463\n",
      "0.0691\n",
      "0.184845\n",
      "0.0589586\n",
      "0.0978751\n",
      "0.121056\n",
      "0.100833\n",
      "0.972841677146\n",
      "51328\n",
      "48554\n",
      "Validation Loss: 0.227084\n",
      "Validation Precision: 0.945955\n",
      "0.115428\n",
      "0.0481783\n",
      "0.0916215\n",
      "0.108747\n",
      "0.149699\n",
      "0.0888952\n",
      "0.100879\n",
      "0.160897\n",
      "0.0451382\n",
      "0.12005\n",
      "0.0247363\n",
      "0.147388\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-b2ae169e1682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m##shuffle the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_correct\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mcorrect_num\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wang/tensorflow_dl/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 698\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    699\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wang/tensorflow_dl/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    890\u001b[0m     results = self._do_run(handle, fetch_handler.targets(),\n\u001b[1;32m    891\u001b[0m                            \u001b[0mfetch_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m    893\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetch_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wang/tensorflow_dl/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 940\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    941\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/wang/tensorflow_dl/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    945\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/wang/tensorflow_dl/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    928\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    #add placeholders\n",
    "    input_placeholder, label_placeholder = add_placeholders()\n",
    "    #add the embed layer\n",
    "    embedding = tf.get_variable('Embedding', [len(word2vec), 50]) \n",
    "    embeds = tf.nn.embedding_lookup(embedding, input_placeholder)\n",
    "    embeds = tf.reshape(embeds, [-1, 150])\n",
    "    #for the hidden unit\n",
    "    W = tf.Variable(tf.random_uniform([150, 100], \n",
    "                                      minval=-np.sqrt(6.0/(150+100)), maxval = np.sqrt(6.0/(150+100))))\n",
    "    b1 = tf.Variable(tf.zeros([100]))\n",
    "    h = tf.nn.tanh(tf.matmul(embeds, W) + b1)\n",
    "    U = tf.Variable(tf.random_uniform([100, 5], \n",
    "                                      minval=-np.sqrt(6.0/(100+5)), maxval = np.sqrt(6.0/(100+5))))\n",
    "    b2 = tf.Variable(tf.zeros([5]))\n",
    "    y = tf.matmul(h, U) + b2\n",
    "    y = tf.nn.dropout(y, 0.9)\n",
    "    pred = tf.nn.softmax(y)\n",
    "    #\n",
    "    eval_correct = evaluation(pred, label_placeholder)\n",
    "    ##regulurization\n",
    "    #L2 = tf.reduce_sum(tf.square(W)) + tf.reduce_sum(tf.square(U))\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, label_placeholder)) + 0.5*0.0001*tf.nn.l2_loss(W)+ 0.5*0.0001*tf.nn.l2_loss(U)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    for epoch in range(max_epoch):\n",
    "        \n",
    "        #shuffle the data\n",
    "        words_indices, tags_matrix = shuffle_data(words_indices, tags_matrix)\n",
    "        words_indices_test, tags_matrix_test = shuffle_data(words_indices_test, tags_matrix_test)\n",
    "        correct_num = []\n",
    "        \n",
    "        for step in range(max_step):\n",
    "            input_batch = words_indices[step*batch_size: (step+1)*batch_size, :]\n",
    "            label_batch = tags_matrix[step*batch_size: (step+1)*batch_size, :]\n",
    "            \n",
    "            #input_batch = indices_to_vec(input_indices_batch, word2vec)\n",
    "            ##shuffle the data\n",
    "            feed_dict = create_feed_dict(input_placeholder, input_batch, label_placeholder, label_batch)\n",
    "            _, loss_iter, correct_tmp = sess.run([train_op, loss, eval_correct], feed_dict)\n",
    "            correct_num.append(correct_tmp)\n",
    "            if step % 200 == 0:\n",
    "                print(loss_iter)\n",
    "        print(np.sum(correct_num)/ (max_step*batch_size))\n",
    "        do_eval(sess, eval_correct,  words_indices_test,\n",
    "                batch_size, input_placeholder, label_placeholder, tags_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-29-3829de0ae4d6>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-3829de0ae4d6>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Validation Precision: 0.832567\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Validation Precision: 0.832567\n",
    "add L2Validation Precision: 0.832450\n",
    "add xavier initialization Validation Precision: 0.832450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(tf.nn.dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9502026184538653"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "48777 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9526184538653366"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "48896/51328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
